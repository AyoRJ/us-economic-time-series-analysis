---
title:  "US Economic Time Series Analysis"
author: "Ayomide Jolaoso"
date:   "Last compiled on:   `r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document: default
fontsize: 12pt
header-includes:
   \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{lmodern}
  \usepackage{lipsum}
  \usepackage{fancyhdr}
  \pagestyle{fancy}
  \renewcommand{\headrulewidth}{0pt}
  \fancyhf{}
  \fancyfoot[C]{\twopagenumbers}
  \fancypagestyle{plain}{
    \renewcommand{\headrulewidth}{0pt}
    \fancyhf{}
    \fancyfoot[C]{\twopagenumbers}
  }
  \usepackage[user]{zref}
  \newcounter{pageaux}
  \def\currentauxref{PAGEAUX1}
  \newcommand{\twopagenumbers}{
    \stepcounter{pageaux}
    \thepageaux\, of\, \ref{\currentauxref}
  }
  \makeatletter
  \newcommand{\resetpageaux}{
    \clearpage
    \edef\@currentlabel{\thepageaux}\label{\currentauxref}
    \xdef\currentauxref{PAGEAUX\thepage}
    \setcounter{pageaux}{0}}
  \AtEndDvi{\edef\@currentlabel{\thepageaux}\label{\currentauxref}}
  \makeatletter
---

<!--
#########################################################
###         DO NOT CHANGE the following code          ###
#########################################################
-->

```{r setup, include=FALSE}
library(forecast)
library(ggplot2)
library(fpp2)
library(corrplot)
library(reshape2)
knitr::opts_chunk$set(echo = TRUE)
```
```{r data-split, echo=FALSE}
data("uschange")
training <- window(uschange, end = c(2014, 4))
test <- window(uschange, start = c(2015, 1))
```

\thispagestyle{empty}
\newpage
\setcounter{page}{1}
\thispagestyle{empty}
\newpage
\setcounter{page}{1}

\begin{abstract}
This report forecasts quarterly U.S. personal consumption expenditure for 2015 Q1 to 2016 Q3 using historical data from 1970 Q1 to 2014 Q4. The \texttt{uschange} dataset includes five economic indicators: Consumption, Income, Production, Savings, and Unemployment. I assessed their stationarity, distribution, patterns, and relationships to identify the most effective forecasting model.
\end{abstract}

\newpage

<!--
#########################################################
###      Start writing your report from line 61       ###
#########################################################
-->

# Forecasting U.S. Consumption Using Economic Indicators 

## Data Exploration
```{r, fig.align="center", fig.width=5.5, fig.height=2.7, echo=FALSE}
autoplot(training, facets=TRUE) +
  ggtitle("Quarterly Time Series of U.S. Economic Indicators") +
  xlab("Time") + ylab("Quarterly % Change") +
  theme_minimal()
```
All five series show stable means over time. Consumption and Income show small fluctuations around zero, while Savings shows larger and sharper volatility, whereas Unemployment and Production show big peaks but were not as erratic as Savings.
```{r, fig.align="center", fig.width=5.5, fig.height=2.7, echo=FALSE, message=FALSE}
library(reshape2)
training_df <- as.data.frame(training)
melted <- melt(training_df)

ggplot(melted, aes(x = value)) +
  geom_histogram(bins = 30, fill = "blue", color = "white") +
  facet_wrap(~ variable, scales = "free", ncol = 3) +
  theme_minimal() +
  ggtitle("Histograms of the Economic Variables")
```
All variables are roughly normally distributed, so no transformations were needed. Savings shows extreme values, indicating volatility. Consumption correlates most strongly with Production (r = 0.55) and Income (r = 0.40), and negatively with Unemployment (r = –0.54). Savings has a weak correlation (r = –0.24).
```{r, fig.align="center", fig.width=5, fig.height=3, echo=FALSE}
library(corrplot)
corrplot(cor(training), method = "color", type = "upper",
         tl.col = "black", addCoef.col = "black", number.cex = 0.7)
```
ADF and KPSS tests confirmed all variables are stationary, so no differencing was required.
```{r, fig.align="center", fig.width=5, fig.height=2.5, echo=FALSE}
decomp <- decompose(ts(training[, "Consumption"], frequency = 4))
plot(decomp)
```
Decomposition of the Consumption series revealed a stable trend and minimal seasonality, indicating that seasonal effects are weak or absent. As a result, I ruled out models that rely on explicit seasonal components, such as Seasonal ARIMA, Holt-Winters Exponential Smoothing, and ETS models, which are more appropriate when strong seasonal patterns are present.

## Model Selection and Forecasting
Autocorrelation analysis showed ACF spikes at lags 1–3 and PACF spikes at lags 1–2, suggesting an MA(3) and AR(1–2) structure. ARIMA(1,0,3) and (2,0,3) were tested, but auto.arima() selected ARIMA(3,0,0)(2,0,0)[4] with the lowest AIC (331.88). Residuals passed diagnostic checks (Ljung-Box p = 0.88), and the model achieved RMSE = 0.27 and Theil’s U = 0.74.

A regression with ARIMA errors using all four predictors further improved fit, with auto.arima() selecting ARIMA(3,1,0)(1,0,0)[4] (AIC = 148.6, RMSE = 0.097, Theil’s U = 0.34). Income had the strongest influence, and Savings,despite its volatility, contributed positively.

A simpler linear regression using the same predictors outperformed both, with RMSE = 0.087, MASE = 0.117, and Theil’s U = 0.25. Residuals were normal and uncorrelated, making it the better model.

```{r lm-forecast-setup, echo=FALSE}
model_lm <- tslm(Consumption ~ Income + Production + Savings + Unemployment, data = training)

futurelm_xreg <- data.frame(
  Income = test[,"Income"],
  Production = test[,"Production"],
  Savings = test[,"Savings"],
  Unemployment = test[,"Unemployment"]
)

forecast_lm <- forecast(model_lm, newdata = futurelm_xreg)
```

```{r, fig.align="center", fig.width=4, fig.height=2, echo=FALSE}
autoplot(forecast_lm) +
  autolayer(test[, "Consumption"], series="Actual") +
  ggtitle("Linear Regression Forecast vs Actual") +
  xlab("Year") + ylab("Consumption % Change") +
  theme_minimal()
```

## Conclusion
Three models were tested to forecast U.S. Consumption: ARIMA, regression with ARIMA errors, and linear regression. While ARIMA captured time-based patterns, the linear model had the best forecast accuracy and offered a strong mix of simplicity, interpretability, and a good performance. Predictors like Income and Production improved accuracy, with residuals meeting model assumptions. The linear regression model was chosen as the final model fit.

## Strengths, Limitations and Potential Improvements
This project applied a thorough approach as it tested stationarity, distribution, autocorrelation, and variable relationships. Comparing ARIMA, regression with ARIMA errors, and linear regression showed that the linear model, despite not modelling residual structure, did the best due to strong, stationary predictors. While the regression-ARIMA model performed well, residuals showed slight autocorrelation. A short test period and the selection of seasonal terms by auto.arima() despite weak visual evidence suggest a possible limitation of overlooked seasonality. A potential improvement could be exploring lagged predictors.



<!--
#########################################################
### DO NOT CHANGE the code until the section 'R code' ###
#########################################################
-->


\newpage
\thispagestyle{empty}
\begin{center}
\Huge \bf [END of the REPORT]
\end{center}


\newpage
<!-- \setcounter{pageaux}{0} -->
<!-- \renewcommand{\thepage}{R-\arabic{page}} -->
\resetpageaux
\renewcommand{\thepageaux}{R-\arabic{pageaux}}


# R code

<!--
#########################################################
###              Start typing from here               ###
#########################################################
-->

\begin{center}
\bf \color{red}
Recall that this section has no page limits, but you are encouraged to be parsimonious (privilege quality over quantity).
\end{center}

# NOTE:
# Some figures shown in the main report were generated from expanded versions of the code below. They were condensed in the report for space efficiency.


```{r}
# LOAD REQUIRED PACKAGES
#libraries that are needed
library(forecast)   
library(fpp2)       
library(fma)        
library(tseries)
```
*EXPLORING THE STRUCTURE OF THE DATASET*
```{r}
#shows the time series plots of all the variables together
plot(uschange)
```
*SPLITTING THE DATA INTO TRAINING AND TESTS*
```{r}
# Training set 1970 -2014, Q1 - Q4
training <- window(uschange, end = c(2014, 4))
# Test set 2015 Q1 – 2016 Q3
test <- window(uschange, start = c(2015, 1))

# Visualising the training and test splits
autoplot(training) +
  ggtitle("Training Data (1970 Q1 - 2014 Q4)") +
  ylab("Quarterly % Change") +
  xlab("Time")

autoplot(test) +
  ggtitle("Test Data (2015 Q1 - 2016 Q3)") +
  ylab("Quarterly % Change") +
  xlab("Time")
```
*STATISTICS AND CORRELATION*
```{r}
#min, max, mean, quartiles for each variable
summary(training)
#Identifies which variables are good predictors through a correlation matrix
cor(training)
```
*Findings:* The correlation matrix shows that Consumption has the strongest positive relationship with Production (0.55) and an okay relationship with Income (0.40). Unemployment is negatively correlated (–0.54)but strong, Savings shows only a weak negative correlation with Consumption (–0.24), which I can infer is due to its high volatility, and changing variance. Based on this, Production and Income are strong candidate predictors for modeling Consumption, and Unemployment and Savings may also be considered.
*DECOMPOSE CONSUMPTION SERIES*

```{r}
#converts to a time series object
consumption_ts <- ts(training[,"Consumption"], frequency = 4, start = c(1970, 1))
# Decompose into trend seasonal and remainder
consumption_decomp <- decompose(consumption_ts) 
#visualises it
plot(consumption_decomp) 
```

*STATIONARY TEST - CONSUMPTION SERIES*
```{r}
adf.test(consumption_ts)                          
kpss.test(consumption_ts)                         

# checks remainder from decomposition
adf.test(na.omit(consumption_decomp$random))
kpss.test(na.omit(consumption_decomp$random))
```
*DISTRIBUTION AND PATTERNS OF CONSUMPTION*
```{r}
#historgram to see distribution
hist(consumption_ts, col = "orange", main = "Histogram of Consumption", xlab = "Consumption")
#Seasonal patterns 
ggseasonplot(consumption_ts)
#ACF and PACF plots
ggtsdisplay(consumption_ts) 
```
*Findings:* The result shows high variability across years, with no consistent or repeating pattern across quarters. This supports the conclusion from the decomposition that the series does not exhibit any strong seasonal effects. Therefore, seasonal ARIMA terms are unlikely to be needed for modelling.

The ACF plot of the Consumption series shows strong autocorrelation at lag 1 and weaker but still significant autocorrelation at lags 2 and 3. The sharp decay after lag 3 suggests a moving average component of order 3.
These patterns indicate that an ARIMA(1,0,2) or ARIMA(1,0,3) model could be suitable for capturing the autocorrelation structure in the data. I have seen that consumption time series is stationary therefore d = 0 as we do not need to apply differencing to the series. 
*EDA FOR REST OF VARIABLES*
```{r}
# Income
income_ts <- ts(training[,"Income"], start = c(1970, 1), frequency = 4)
plot(income_ts, main = "Income Time Series", ylab = "Income", xlab = "Time")

#stationary tests
adf.test(income_ts)
kpss.test(income_ts)

#historgram to see distribution
hist(income_ts, col = "grey", main = "Histogram of Income", xlab = "Income")

# Production
production_ts <- ts(training[,"Production"], start = c(1970, 1), frequency = 4)
plot(production_ts, main = "Production Time Series", ylab = "Production", xlab = "Time")

#stationary tests
adf.test(production_ts)
kpss.test(production_ts)

#histogram to see distribution
hist(production_ts, col = "green", main = "Histogram of Production", xlab = "Production")

# Savings
savings_ts <- ts(training[,"Savings"], start = c(1970, 1), frequency = 4)
plot(savings_ts, main = "Savings Time Series", ylab = "Savings", xlab = "Time")

#stationary tests
adf.test(savings_ts)
kpss.test(savings_ts)

#histogram to see distribution
hist(savings_ts, col = "blue", main = "Histogram of Savings", xlab = "Savings")

# Unemployment
unemployment_ts <- ts(training[,"Unemployment"], start = c(1970, 1), frequency = 4)
plot(unemployment_ts, main = "Unemployment Time Series", ylab = "Unemployment", xlab = "Time")

#test for stationary
adf.test(unemployment_ts)
kpss.test(unemployment_ts)

#histogram
hist(unemployment_ts, col = "purple", main = "Histogram of Unemployment", xlab = "Unemployment")
```
*Findings:* Stationary was tested using the Augmented Dickey-Fuller (ADF) and KPSS tests, both confirming that all series were stationary at the 5% level. Histograms supported that most variables were evenly and well distributed around the centre.
*Statistical Model 1 TEST: ARIMA on Consumption Only*

*Rationale:* An ARIMA model was selected to see the autocorrelation in the Consumption series, which appeared stationary and showed no strong seasonal patterns but showed some autocorrelation in its ACF and PACF plots.

```{r}
#Selecting ARIMA model, doing auto, and manual ones I have selected
arima_auto <- auto.arima(consumption_ts)                        
arima_manual <- Arima(consumption_ts, order = c(1, 0, 3))       
arima_manual2 <- Arima(consumption_ts, order = c(2, 0, 3))      

# Comparing AIC values
AIC(arima_auto); AIC(arima_manual); AIC(arima_manual2)          

# Checking the coefficients and fit statistics
summary(arima_auto)
# Residual plots and Ljung-Box test
checkresiduals(arima_auto)  
# Q-Q plot of residuals
qqnorm(residuals(arima_auto))
qqline(residuals(arima_auto)) 

# Forecasting the chosen ARIMA model (auto)
fcast_arima <- forecast(arima_auto, h = 7)
autoplot(fcast_arima) +
  autolayer(test[,"Consumption"], series = "Actual", color = "red") +
  ggtitle("ARIMA Forecast vs Actual Consumption") +
  ylab("Consumption") +
  xlab("Year") +
  theme_minimal()

# The forecast accuracy metrics
accuracy(fcast_arima, test[,"Consumption"])
```
*Findings:* I selected ARIMA(1,0,3) and ARIMA(2,0,3) as manual candidate models, avoiding ARIMA(3,0,3) to reduce the risk of overfitting with too many high-order terms. These will be compared against a model selected by auto.arima()

The auto.arima() came out as the best model as its AIC was lower than the other 2 I tested. The function selected an ARIMA(3,0,0)(2,0,0)[4] model with a non-zero mean. Although visual inspection did not reveal clear seasonal patterns, the auto.arima() function selected a model with seasonal autoregressive terms. 

Coefficients were all statistically significant and stable. The training residuals showed no autocorrelation, indicating that the model fits well. This model outperformed manual ARIMA alternatives based on AIC and is selected as model to compare to other types of statistical models.
## Statistical Model 2: Regression with ARIMA Errors
*Rationale:* A regression model with ARIMA errors was implemented to evaluate whether including external predictors while also modeling autocorrellated residuals would improve the predictive performance.
```{r}
# Trying different combinations of predictors
fit_m2a <- auto.arima(consumption_ts, xreg = cbind(income_ts, production_ts))
fit_m2b <- auto.arima(consumption_ts, xreg = cbind(income_ts, production_ts, unemployment_ts))
fit_m2c <- auto.arima(consumption_ts, xreg = cbind(income_ts, production_ts, unemployment_ts, savings_ts))

# Comparing model fit
AIC(fit_m2a); BIC(fit_m2a)
AIC(fit_m2b); BIC(fit_m2b)
AIC(fit_m2c); BIC(fit_m2c)

summary(fit_m2c)
checkresiduals(fit_m2c)

# Forecast from best regression ARIMA model picked based on AIC/BIC
future_xreg <- cbind(
  income_ts = test[,"Income"],
  production_ts = test[,"Production"],
  unemployment_ts = test[,"Unemployment"],
  savings_ts = test[,"Savings"]
)

forecast_m2c <- forecast(fit_m2c, xreg = future_xreg)

autoplot(forecast_m2c) +
  autolayer(test[,"Consumption"], series = "Actual", color = "red") +
  ggtitle("Regression with ARIMA Errors Forecast vs Actual Consumption") +
  ylab("Consumption") +
  xlab("Year")

accuracy(forecast_m2c, test[,"Consumption"])

```
*Findings:* Model 2c (with all 4 predictors) has by far the lowest AIC and BIC, Unemployment and Savings do appear to contribute to forecasting Consumption. Especially adding Savings, despite its weak correlation, significantly improved model fit. The sharp AIC/BIC drop for model 2c justifies its inclusion.

The residuals are  centered around zero with a constant variance. The histogram closely matches a normal distribution, suggesting residuals are okay. The ACF plot shows most autocorrolations fall within the confidence bands — but a few are close to the edge. Since p-value < 0.05 in the L box test, this indicates that some autocorrelation remains in the residuals meaning residuals may not be pure white noise. So despite good AIC/BIC performance the model has slightly concerning autocorrelation in residuals.

The regression model with ARIMA(3,1,0)(1,0,0)[4] errors shows a significantly better fit than the ARIMA-only model, with a much lower AIC (148.57 vs. 331.88) and lower training RMSE (0.34 vs. 0.58).

The forecast lines closely follow the actual red line with narrow confidence bands

## Statistical Model 3: Linear Regression Model
*Rationale:* This model was used to see if predictors like Income and Production, which showed strong correlations with Consumption, and adding uneployment and savings, as it was found these variables could improve the model, could help forecast it without needing to model patterns in the residuals


```{r}
# Simple linear regression model
model_lm <- tslm(consumption_ts ~ income_ts + production_ts + unemployment_ts + savings_ts)
checkresiduals(model_lm)

# Forecasting using the test predictors
futurelm_xreg <- data.frame(
  income_ts = test[,"Income"],
  production_ts = test[,"Production"],
  unemployment_ts = test[,"Unemployment"],
  savings_ts = test[,"Savings"]
)

forecast_lm <- forecast(model_lm, newdata = futurelm_xreg)

autoplot(forecast_lm) +
  autolayer(test[,"Consumption"], series = "Actual", color = "red") +
  ggtitle("Linear Regression Forecast vs Actual Consumption") +
  ylab("Consumption % Change") +
  xlab("Year")

accuracy(forecast_lm, test[,"Consumption"])

```
*Findings:* The residuals from the linear regression model appear to be fairly well-behaved. The residuals fluctuate around zero with constant variance.
The linear regression model produced accurate forecasts, especially in the test period. The test set RMSE (0.087) and MASE (0.117) are both slightly better than those from the regression with ARIMA errors and significantly better than the ARIMA only model. The Theil’s U statistic (0.25) also indicates strong accuracy. While the model does not account for autocorrelation, its strong test performance supports that simpler models can sometimes generalise better when predictors are strong.

